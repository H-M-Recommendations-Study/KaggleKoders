{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a47f230f-1253-445e-9f2f-c8bac4edeaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31788324, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_dat</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>0663713001</td>\n",
       "      <td>0.050831</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>0541518023</td>\n",
       "      <td>0.030492</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>00007d2de826758b65a93dd24ce629ed66842531df6699...</td>\n",
       "      <td>0505221004</td>\n",
       "      <td>0.015237</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>00007d2de826758b65a93dd24ce629ed66842531df6699...</td>\n",
       "      <td>0685687003</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>00007d2de826758b65a93dd24ce629ed66842531df6699...</td>\n",
       "      <td>0685687004</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        t_dat                                        customer_id  article_id  \\\n",
       "0  2018-09-20  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...  0663713001   \n",
       "1  2018-09-20  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...  0541518023   \n",
       "2  2018-09-20  00007d2de826758b65a93dd24ce629ed66842531df6699...  0505221004   \n",
       "3  2018-09-20  00007d2de826758b65a93dd24ce629ed66842531df6699...  0685687003   \n",
       "4  2018-09-20  00007d2de826758b65a93dd24ce629ed66842531df6699...  0685687004   \n",
       "\n",
       "      price  sales_channel_id  \n",
       "0  0.050831                 2  \n",
       "1  0.030492                 2  \n",
       "2  0.015237                 2  \n",
       "3  0.016932                 2  \n",
       "4  0.016932                 2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"transactions_train.csv\", dtype={\"article_id\": str})\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "676bba6a-52ca-4ee7-9fa4-f454ec6b27b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2020-09-22 00:00:00')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t_dat 열을 날짜 형식으로 변환하고 데이터셋의 최신 날짜를 확인함\n",
    "df[\"t_dat\"] = pd.to_datetime(df[\"t_dat\"])\n",
    "df[\"t_dat\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3507fd70-b66c-4d6e-919c-33b12a0e646d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72581, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최근 활성화된 고객들을 필터링함\n",
    "active_articles = df.groupby(\"article_id\")[\"t_dat\"].max().reset_index()\n",
    "active_articles = active_articles[active_articles[\"t_dat\"] >= \"2019-09-01\"].reset_index()\n",
    "active_articles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9bd70ca-14c8-4c32-bee5-361194cb7c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29634404, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df[\"article_id\"].isin(active_articles[\"article_id\"])].reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4251962c-ebac-40ca-b03d-bc1334e21866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "week\n",
       "65     620104\n",
       "13     549443\n",
       "42     518403\n",
       "12     517428\n",
       "64     508664\n",
       "        ...  \n",
       "93     174190\n",
       "102    164298\n",
       "104    163143\n",
       "97     162580\n",
       "94     152807\n",
       "Name: count, Length: 105, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"week\"] = (df[\"t_dat\"].max() - df[\"t_dat\"]).dt.days // 7\n",
    "df[\"week\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d13b7aef-bc6f-4cbb-8f54-d4fd2fd2e4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.2)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.3.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.5.0 scipy-1.13.1 threadpoolctl-3.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cb54d02-4bf1-4696-b244-fe18df13ad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고객ID를 정수형 코드로 변환해서 모델이 처리하기 쉽게 함\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "article_ids = np.concatenate([[\"placeholder\"], np.unique(df[\"article_id\"].values)])\n",
    "\n",
    "le_article = LabelEncoder()\n",
    "le_article.fit(article_ids)\n",
    "df[\"article_id\"] = le_article.transform(df[\"article_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f2ab8e7-8733-45ea-a7da-91cc602fb2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300129, 5), (68984, 5))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습, 검증을 위한 데이터셋 생성\n",
    "WEEK_HIST_MAX = 5\n",
    "\n",
    "# 목적: 지정된 week에 대한 훈련 데이터를 생성하고 해당 주차를 예측하는 데 사용될 고객의 과거 구매이력을 포함하는 데이터셋 생성\n",
    "def create_dataset(df, week):\n",
    "    # 지정된 주차 'week' 이후의 데이터를 수집하여 최대 5주간의 구매 이력을 추출함\n",
    "    hist_df = df[(df[\"week\"] > week) & (df[\"week\"] <= week + WEEK_HIST_MAX)]\n",
    "    # 고객별로 그룹화하여 'article_id'와 'week' 정보를 리스트 형태로 저장\n",
    "    hist_df = hist_df.groupby(\"customer_id\").agg({\"article_id\": list, \"week\": list}).reset_index()\n",
    "    hist_df.rename(columns={\"week\": 'week_history'}, inplace=True)\n",
    "\n",
    "    # 지정된 'week'에서의 고객별 구매 내역을 타겟으로 설정함\n",
    "    target_df = df[df[\"week\"] == week]\n",
    "    # 고객별로 그룹화하여 'article_id'를 리스트로 저장. 해당 주차에 고객이 구매한 제품 목록임\n",
    "    target_df = target_df.groupby(\"customer_id\").agg({\"article_id\": list}).reset_index()\n",
    "    target_df.rename(columns={\"article_id\": \"target\"}, inplace=True)\n",
    "    target_df[\"week\"] = week\n",
    "    \n",
    "    # 과거 데이터와 타겟 데이터를 'customer_id'를 기준으로 조인함. \n",
    "    return target_df.merge(hist_df, on=\"customer_id\", how=\"left\")\n",
    "\n",
    "# 0주차에 대한 데이터셋을 생성하여 모델 평가에 사용함\n",
    "val_weeks = [0]\n",
    "# 1~4주차에 대한 데이터셋 생성하여 모델 학습에 사용함\n",
    "train_weeks = [1, 2, 3, 4]\n",
    "\n",
    "# create_dataset 함수 사용하여 데이터들을 병합하여 최종적인 데이터셋 형성\n",
    "val_df = pd.concat([create_dataset(df, w) for w in val_weeks]).reset_index(drop=True)\n",
    "train_df = pd.concat([create_dataset(df, w) for w in train_weeks]).reset_index(drop=True)\n",
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70ab1b99-f49f-4616-8357-d9921fce3522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m986.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d5c6617-87ab-476a-9cb4-10147986b646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0, 70912, 69932, 70149, 14648,\n",
       "         11949, 66254, 66254, 67809]),\n",
       " tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 0.2000, 0.2000, 0.2000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch 사용하여 사용자 정의 데이터셋과 데이터 로더를 설정하고 모델 훈련시킴\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "class HMDataset(Dataset):\n",
    "    def __init__(self, df, seq_len, is_test=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.seq_len = seq_len\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    # 데이터 포매팅 및 전처리 로직 구현\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "\n",
    "        if self.is_test:\n",
    "            # 빈 타겟 텐서를 생성함\n",
    "            target = torch.zeros(2).float()\n",
    "        else:\n",
    "            # 빈 타겟 텐서를 생성하고 row.target에 포함된 각 아이템에 대해 해당 인덱스를 1로 설정함(원핫인코딩방식)\n",
    "            target = torch.zeros(len(article_ids)).float()\n",
    "            for t in row.target:\n",
    "                target[t] = 1.0\n",
    "\n",
    "        # article_hist(고객의 과거 구매 ID)와 week_hist(해당 구매가 발생한 시간의 상대적 차이) 텐서 생성.\n",
    "        article_hist = torch.zeros(self.seq_len).long()\n",
    "        week_hist = torch.ones(self.seq_len).float()\n",
    "\n",
    "        if isinstance(row.article_id, list):\n",
    "            if len(row.article_id) >= self.seq_len:\n",
    "                # 가장 최근의 seq_len개의 article_id와 week_history 추출\n",
    "                article_hist = torch.LongTensor(row.article_id[-self.seq_len:])\n",
    "                week_hist = (torch.LongTensor(row.week_history[-self.seq_len:]) - row.week)/WEEK_HIST_MAX/2\n",
    "            else:\n",
    "                # 필요한 길이만큼만 데이터를 할당하고 나머지는 0 또는 1로 채움 (시퀀스 데이터 처리에서 일반적으로 사용되는 방법임)\n",
    "                article_hist[-len(row.article_id):] = torch.LongTensor(row.article_id)\n",
    "                week_hist[-len(row.article_id):] = (torch.LongTensor(row.week_history) - row.week)/WEEK_HIST_MAX/2\n",
    "\n",
    "        return article_hist, week_hist, target\n",
    "\n",
    "# val_df 데이터 프레임을 사용하여 인스턴스 생성하고 시퀀스 길이를 64로 설정한 뒤 인덱스 1 데이터 불러와서 확인함\n",
    "HMDataset(val_df, 64)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e79bdc8-fd05-4cb9-aae5-28f6d02a1705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 목적: 주어진 epoch에 따라 optimizer의 학습률을 조정함\n",
    "def adjust_lr(optimizer, epoch):\n",
    "    if epoch < 1:\n",
    "        # 학습 초기에 매우 낮은 학습률로 시작함\n",
    "        lr = 5e-5\n",
    "    elif epoch < 6:\n",
    "        # 학습률을 증가시켜 빠른 학습 추진함\n",
    "        lr = 1e-3\n",
    "    elif epoch < 9:\n",
    "        # 학습률을 감소시켜 미세하게 조정함\n",
    "        lr = 1e-4\n",
    "    else:\n",
    "        # 매우 낮은 학습률로 마무리 단계에서 세밀하게 조정함\n",
    "        lr = 1e-5\n",
    "\n",
    "    # 설정된 lr을 optimizer의 모든 매개변수 그룹에 적용함. \n",
    "    for p in optimizer.param_groups:\n",
    "        p['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "# 목적: 주어진 신경망 모델(net)에 대해 Adam 최적화 도구를 설정하고 반환함.\n",
    "# Adam최적화: 자동으로 학습률을 조정하면서 모멘텀을 사용하여 최적화를 수행하는 알고리즘\n",
    "def get_optimizer(net):\n",
    "    # requires_grad가 True인 모델 파라미터만 최적화 대상으로 선택함(학습 가능한 파라미터만 최적화하겠다는 뜻)\n",
    "    # betas = (0.9, 0.999): Adam 최적화의 두 모멘텀 파라미터 beta1과 beta2\n",
    "    # eps = 1e-08: 수치안정성을 위한 작은 상수\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=3e-4, betas=(0.9, 0.999),\n",
    "                                 eps=1e-08)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6396657",
   "metadata": {},
   "source": [
    "### 신경망 아키텍쳐 HMModel를 정의하고 초기화 하고 실행하는 과정임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a3e1028-8d7a-4744-9e2b-9c0ffffa8323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HMModel(nn.Module):\n",
    "\n",
    "    # 초기화 함수\n",
    "    def __init__(self, article_shape):\n",
    "        super(HMModel, self).__init__()\n",
    "        # article_emb: article_id를 벡터로 변환하는데 사용됨. 각 article_id를 고차원 공간에서의 밀집 벡터로 표현하여 아티클 간의 관계를 학습하는데 도움을 줌\n",
    "        self.article_emb = nn.Embedding(article_shape[0], embedding_dim=article_shape[1])\n",
    "        # article_likelihood: 각 article의 구매 가능성을 나타내는 학습 가능한 파라미터로, 초기값은 모두 0임\n",
    "        self.article_likelihood = nn.Parameter(torch.zeros(article_shape[0]), requires_grad=True)\n",
    "        # top: 컨볼루션 레이어. 입력 특성을 변환하기 위한 일련의 1D 컨볼루션 레이어와 LeakyReLU 활성화 함수를 포함함. 최종 출력을 생성하기 전에 특성들을 더 추출하고 변환함\n",
    "        self.top = nn.Sequential(nn.Conv1d(3, 32, kernel_size=1), nn.LeakyReLU(),\n",
    "                                 nn.Conv1d(32, 8, kernel_size=1), nn.LeakyReLU(),\n",
    "                                 nn.Conv1d(8, 1, kernel_size=1))\n",
    "    \n",
    "    # 순전파 함수\n",
    "    # 단계: 임베딩 레이어 -> 유사성 계산 -> 통계적 처리 -> 특성 병합과 컨볼루션 처리\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # 1. 입력데이터 분리\n",
    "        article_hist, week_hist = inputs[0], inputs[1]\n",
    "\n",
    "        # 2. 임베딩 레이어를 통한 처리\n",
    "        # article_hist가 임베딩 레이어 self.article_emb를 통해 각 article_id를 해당하는 벡터로 변환함\n",
    "        x = self.article_emb(article_hist)\n",
    "        # 변환된 벡터는 F.normalize 함수를 통해 정규화됨\n",
    "        # -> 벡터의 길이를 1로 맞춰서 다음 계산에서 수치적 안정성을 높이는 역할임\n",
    "        x = F.normalize(x, dim=2)\n",
    "\n",
    "        # 3. 임베딩 가중치와의 내적\n",
    "        # 정규화된 임베딩벡터 x와 임베딩레이어의 가중치 self.article_emb.weight의 전치와 내적을 계산함.\n",
    "        # -> 각 벡터가 다른 벡터들과 얼마나 유사한지 평가하는 점수를 생성함\n",
    "        x = x@F.normalize(self.article_emb.weight).T\n",
    "\n",
    "        # 4. 최대 유사성 추출\n",
    "        # 각 샘플에 대해 가장 높은 유사성 점수와 해당 인덱스를 추출함\n",
    "        x, indices = x.max(axis=1)\n",
    "        # clamp 함수를 사용해 점수를 [0.001, 0.999] 범위로 제한함\n",
    "        # -> 로그 연산시 발생할 수 있는 수치 문제를 방지하는 역할임\n",
    "        x = x.clamp(1e-3, 0.999)\n",
    "        # 로지스틱 시그모이드 함수의 역함수를 적용하여 로짓변환함\n",
    "        x = -torch.log(1/x - 1)\n",
    "\n",
    "        # 5. week 정보 집계\n",
    "        # week_hist에서 incides에 해당하는 최대 유사성의 week 정보를 추출함\n",
    "        max_week = week_hist.unsqueeze(2).repeat(1, 1, x.shape[-1]).gather(1, indices.unsqueeze(1).repeat(1, week_hist.shape[1], 1))\n",
    "        # 이를 평균내어 해당 샘플의 평균 구매 시점을 계산함\n",
    "        max_week = max_week.mean(axis=1).unsqueeze(1)\n",
    "\n",
    "        # 6. 특성 병합 및 최정 컨볼루션 레이어 처리\n",
    "        # torch.cat를 사용해 특성들을 병합함\n",
    "        x = torch.cat([x.unsqueeze(1), max_week,\n",
    "                       self.article_likelihood[None, None, :].repeat(x.shape[0], 1, 1)], axis=1)\n",
    "        # 병합된 특성을 컨볼루션레이어 시퀀스 self.top에 통과시켜서 최종 출력을 생성함\n",
    "        x = self.top(x).squeeze(1)\n",
    "        return x\n",
    "\n",
    "# HMModel 인스턴스를 생성하고 article_id의 수와 임베딩 차원을 설정함\n",
    "model = HMModel((len(le_article.classes_), 512))\n",
    "# 모델을 GPU로 이동시킴\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96791fbc",
   "metadata": {},
   "source": [
    "### 신경망 모델을 검증하는 과정 설명함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c456efa1-d9e3-4934-9219-df6dd87f3322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# 목적: 예측과 실제를 비교해서 주어진 k값에 대해 평균정밀도(MAP)를 계산함\n",
    "# topk_preds: 모델이 예측한 상위 k개의 결과 인덱스 리스트\n",
    "def calc_map(topk_preds, target_array, k=12):\n",
    "    # 각 예측에 대한 정밀도를 저장할 리스트\n",
    "    metric = []\n",
    "    # true positives, false positives\n",
    "    tp, fp = 0, 0\n",
    "\n",
    "    for pred in topk_preds:\n",
    "        # 인덱스에 해당하는 타겟 값이 참(1)이면(=실제로 양성이면)\n",
    "        if target_array[pred]:\n",
    "            # tp 값 1만큼 증가시킴\n",
    "            tp += 1\n",
    "            # metric 리스트에 현재까지의 정밀도를 추가함\n",
    "            metric.append(tp/(tp + fp))\n",
    "        else:\n",
    "            # 아니면 fp값 1만큼 증가시킴\n",
    "            fp += 1\n",
    "    # 계산된 모든 정밀도 값의 합을 k와 실제 양성 샘플수의 최솟값으로 나눔\n",
    "    # min을 한 이유는 실제 양성 샘플의 수가 k보다 적을 수 있기 때문\n",
    "    return np.sum(metric) / min(k, target_array.sum())\n",
    "\n",
    "# 배치로부터 입력데이터와 타겟데이터를 분리해서 GPU로 이동시킴\n",
    "def read_data(data):\n",
    "    # 입력 데이터 튜플과 타겟데이터를 반환함\n",
    "    return tuple(d.cuda() for d in data[:-1]), data[-1].cuda()\n",
    "\n",
    "# 주어진 모델을 사용하여 검증 데이터셋에서 성능을 평가하고 평균정밀도(MAP)을 계산하는 함수\n",
    "def validate(model, val_loader, k=12):\n",
    "    # 모델을 평가모드로 설정함\n",
    "    model.eval()\n",
    "\n",
    "    tbar = tqdm(val_loader, file=sys.stdout)\n",
    "\n",
    "    maps = []\n",
    "    # torch.no_grad(): 자동미분기능을 비활성화해서 메모리 사용을 줄이고 계산 속도를 향상시킴\n",
    "    with torch.no_grad():\n",
    "        # 데이터 로더에서 배치 데이터를 순차적으로 가져옴\n",
    "        for idx, data in enumerate(tbar):\n",
    "            # 각 배치 데이터를 읽고 GPU로 이동시킴\n",
    "            inputs, target = read_data(data)\n",
    "            # 모델에 입력데이터를 전달하여 로짓(에측 결과의 원시점수)를 계산함\n",
    "            logits = model(inputs)\n",
    "            # 로짓 중 확신도? 상위 k개를 선택함 (확신도: 모델이 예측한 각 분류결과에 대해서 얼마나 자신있는지..? 정답일 가능성)\n",
    "            _, indices = torch.topk(logits, k, dim=1)\n",
    "            # GPU에서 계산된 결과를 CPU로 옮기고 넘파이 배열로 변환함\n",
    "            indices = indices.detach().cpu().numpy()\n",
    "            target = target.detach().cpu().numpy()\n",
    "            # 각 예측에 대해 MAP를 계산하고 리스트에 추가함\n",
    "            for i in range(indices.shape[0]):\n",
    "                maps.append(calc_map(indices[i], target[i]))\n",
    "\n",
    "    # 계산된 모든 MAP 값의 평균을 반환함. 평균값은 모델의 전반적인 성능을 나타냄\n",
    "    return np.mean(maps)\n",
    "\n",
    "# 모델이 처리할 각 입력데이터의 길이 또는 시퀀스의 요소 수 지정함\n",
    "SEQ_LEN = 16\n",
    "# 배치사이즈\n",
    "BS = 32\n",
    "# DataLoader에서 사용할 작업자(Worker)의 수. 데이터로딩을 위해 동시에 실행할 프로세스의 수\n",
    "NW = 2\n",
    "\n",
    "# 모델학습과 검증에 적합한 데이터 형태로 변환함\n",
    "val_dataset = HMDataset(val_df, SEQ_LEN)\n",
    "# Dataset에서 제공하는 데이터를 모델이 사용할 수 있도록 배치 단위로 묶어주는 역할을 함\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=BS,\n",
    "                        shuffle=False, # 데이터를 섞지 않고 순차적으로 로드함\n",
    "                        num_workers=NW,\n",
    "                        pin_memory=False, # DataLoader가 텐서를 CUDA 고정 메모리에 올리지 않도록 함. 데이터를 GPU로 더 빠르게 전송할 수 있게 해줌\n",
    "                        drop_last=False # 마지막 배치가 설정된 배치사이즈보다 작을 경우 이를 버리지 않고 사용함\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832cb33f",
   "metadata": {},
   "source": [
    "### 손실함수(dice_loss) 구현하고 신경망모델 훈련하고 검증함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f905add1-da13-4a29-b3c2-e4d1a9ce0fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 99.4574 lr: 5e-05: 100%|██████████████████████████████████████████████| 9379/9379 [06:46<00:00, 23.09it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:46<00:00, 46.11it/s]\n",
      "Epoch 1\n",
      "Train Loss: 99.4574\n",
      "Validation MAP: 0.023463344107770078\n",
      "\n",
      "Epoch 2 Loss: 99.3079 lr: 0.001: 100%|██████████████████████████████████████████████| 9379/9379 [06:36<00:00, 23.64it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:46<00:00, 46.82it/s]\n",
      "Epoch 2\n",
      "Train Loss: 99.3079\n",
      "Validation MAP: 0.023983598739893707\n",
      "\n",
      "Epoch 3 Loss: 99.2424 lr: 0.001: 100%|██████████████████████████████████████████████| 9379/9379 [06:36<00:00, 23.63it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:48<00:00, 44.64it/s]\n",
      "Epoch 3\n",
      "Train Loss: 99.2424\n",
      "Validation MAP: 0.02311642030903419\n",
      "\n",
      "Epoch 4 Loss: 99.2212 lr: 0.001: 100%|██████████████████████████████████████████████| 9379/9379 [06:38<00:00, 23.55it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:46<00:00, 46.24it/s]\n",
      "Epoch 4\n",
      "Train Loss: 99.2212\n",
      "Validation MAP: 0.023058467315314645\n",
      "\n",
      "Epoch 5 Loss: 99.2004 lr: 0.001: 100%|██████████████████████████████████████████████| 9379/9379 [06:45<00:00, 23.10it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:47<00:00, 45.09it/s]\n",
      "Epoch 5\n",
      "Train Loss: 99.2004\n",
      "Validation MAP: 0.023266210282059687\n",
      "\n",
      "Epoch 6 Loss: 99.1767 lr: 0.001: 100%|██████████████████████████████████████████████| 9379/9379 [08:11<00:00, 19.08it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [01:03<00:00, 34.11it/s]\n",
      "Epoch 6\n",
      "Train Loss: 99.1767\n",
      "Validation MAP: 0.022830309336845347\n",
      "\n",
      "Epoch 7 Loss: 99.1297 lr: 0.0001: 100%|█████████████████████████████████████████████| 9379/9379 [08:31<00:00, 18.33it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [01:01<00:00, 34.94it/s]\n",
      "Epoch 7\n",
      "Train Loss: 99.1297\n",
      "Validation MAP: 0.022518378368877167\n",
      "\n",
      "Epoch 8 Loss: 99.1204 lr: 0.0001: 100%|█████████████████████████████████████████████| 9379/9379 [07:01<00:00, 22.27it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:46<00:00, 45.99it/s]\n",
      "Epoch 8\n",
      "Train Loss: 99.1204\n",
      "Validation MAP: 0.022430095129194638\n",
      "\n",
      "Epoch 9 Loss: 99.1108 lr: 0.0001: 100%|█████████████████████████████████████████████| 9379/9379 [06:52<00:00, 22.72it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:46<00:00, 46.15it/s]\n",
      "Epoch 9\n",
      "Train Loss: 99.1108\n",
      "Validation MAP: 0.02261133131103963\n",
      "\n",
      "Epoch 10 Loss: 99.0993 lr: 1e-05: 100%|█████████████████████████████████████████████| 9379/9379 [06:50<00:00, 22.82it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:46<00:00, 46.03it/s]\n",
      "Epoch 10\n",
      "Train Loss: 99.0993\n",
      "Validation MAP: 0.022389141779356737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 딥러닝에서 자주 사용되는 손실함수중 하나인 Dice Loss를 구현한거임. 주로 두 샘플 간의 유사성을 측정하는데 사용되고 특히 불균형한 데이터셋에 유영함.\n",
    "# 예측결과와 실제값 사이의 유사도를 계산하여 두 데이터의 겹치는 부분이 클수록 손실을 줄임\n",
    "def dice_loss(y_pred, y_true):\n",
    "    # 모델의 원시출력\n",
    "    y_pred = y_pred.sigmoid()\n",
    "    intersect = (y_true*y_pred).sum(axis=1)\n",
    "\n",
    "    return 1 - (intersect/(intersect + y_true.sum(axis=1) + y_pred.sum(axis=1))).mean()\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs):\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    optimizer = get_optimizer(model)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        tbar = tqdm(train_loader, file=sys.stdout)\n",
    "\n",
    "        lr = adjust_lr(optimizer, e)\n",
    "\n",
    "        loss_list = []\n",
    "\n",
    "        for idx, data in enumerate(tbar):\n",
    "            inputs, target = read_data(data)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits, target) + dice_loss(logits, target)\n",
    "\n",
    "\n",
    "            #loss.backward()\n",
    "            scaler.scale(loss).backward()\n",
    "            #optimizer.step()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            loss_list.append(loss.detach().cpu().item())\n",
    "\n",
    "            avg_loss = np.round(100*np.mean(loss_list), 4)\n",
    "\n",
    "            tbar.set_description(f\"Epoch {e+1} Loss: {avg_loss} lr: {lr}\")\n",
    "\n",
    "        val_map = validate(model, val_loader)\n",
    "\n",
    "        log_text = f\"Epoch {e+1}\\nTrain Loss: {avg_loss}\\nValidation MAP: {val_map}\\n\"\n",
    "\n",
    "        print(log_text)\n",
    "\n",
    "        #logfile = open(f\"models/{MODEL_NAME}_{SEED}.txt\", 'a')\n",
    "        #logfile.write(log_text)\n",
    "        #logfile.close()\n",
    "    return model\n",
    "\n",
    "\n",
    "MODEL_NAME = \"exp001\"\n",
    "SEED = 0\n",
    "\n",
    "train_dataset = HMDataset(train_df, SEQ_LEN)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BS, shuffle=True, num_workers=NW,\n",
    "                          pin_memory=True, drop_last=True)\n",
    "\n",
    "model = train(model, train_loader, val_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "696ed250-d7bf-44b0-b4f8-24d35687268b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 99.1118 lr: 5e-05: 100%|██████████████████████████████████████████████| 9283/9283 [08:12<00:00, 18.83it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:51<00:00, 41.64it/s]\n",
      "Epoch 1\n",
      "Train Loss: 99.1118\n",
      "Validation MAP: 0.02281379879790295\n",
      "\n",
      "Epoch 2 Loss: 99.1198 lr: 0.001: 100%|██████████████████████████████████████████████| 9283/9283 [08:26<00:00, 18.33it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:51<00:00, 41.53it/s]\n",
      "Epoch 2\n",
      "Train Loss: 99.1198\n",
      "Validation MAP: 0.02518466185549604\n",
      "\n",
      "Epoch 3 Loss: 99.0855 lr: 0.001: 100%|██████████████████████████████████████████████| 9283/9283 [08:27<00:00, 18.31it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:51<00:00, 41.53it/s]\n",
      "Epoch 3\n",
      "Train Loss: 99.0855\n",
      "Validation MAP: 0.025543896871232236\n",
      "\n",
      "Epoch 4 Loss: 99.0593 lr: 0.001: 100%|██████████████████████████████████████████████| 9283/9283 [08:27<00:00, 18.30it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:52<00:00, 41.41it/s]\n",
      "Epoch 4\n",
      "Train Loss: 99.0593\n",
      "Validation MAP: 0.02601509394211474\n",
      "\n",
      "Epoch 5 Loss: 99.0456 lr: 0.001: 100%|██████████████████████████████████████████████| 9283/9283 [07:52<00:00, 19.65it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:49<00:00, 43.43it/s]\n",
      "Epoch 5\n",
      "Train Loss: 99.0456\n",
      "Validation MAP: 0.026751584260624288\n",
      "\n",
      "Epoch 6 Loss: 99.0308 lr: 0.001: 100%|██████████████████████████████████████████████| 9283/9283 [07:32<00:00, 20.50it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:47<00:00, 45.56it/s]\n",
      "Epoch 6\n",
      "Train Loss: 99.0308\n",
      "Validation MAP: 0.02592002799168003\n",
      "\n",
      "Epoch 7 Loss: 98.9812 lr: 0.0001: 100%|█████████████████████████████████████████████| 9283/9283 [07:10<00:00, 21.54it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:44<00:00, 48.27it/s]\n",
      "Epoch 7\n",
      "Train Loss: 98.9812\n",
      "Validation MAP: 0.02676772710276785\n",
      "\n",
      "Epoch 8 Loss: 98.9712 lr: 0.0001: 100%|█████████████████████████████████████████████| 9283/9283 [08:22<00:00, 18.47it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:51<00:00, 41.64it/s]\n",
      "Epoch 8\n",
      "Train Loss: 98.9712\n",
      "Validation MAP: 0.026881899865073854\n",
      "\n",
      "Epoch 9 Loss: 98.9627 lr: 0.0001: 100%|█████████████████████████████████████████████| 9283/9283 [08:20<00:00, 18.55it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:51<00:00, 41.72it/s]\n",
      "Epoch 9\n",
      "Train Loss: 98.9627\n",
      "Validation MAP: 0.02702504391570144\n",
      "\n",
      "Epoch 10 Loss: 98.9523 lr: 1e-05: 100%|█████████████████████████████████████████████| 9283/9283 [08:20<00:00, 18.57it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:51<00:00, 41.51it/s]\n",
      "Epoch 10\n",
      "Train Loss: 98.9523\n",
      "Validation MAP: 0.026738734314082104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_df = pd.concat([train_df[train_df[\"week\"] < 4], val_df])\n",
    "train_dataset = HMDataset(combined_df, SEQ_LEN)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BS, shuffle=True, num_workers=NW,\n",
    "                          pin_memory=False, drop_last=True)\n",
    "model = train(model, train_loader, val_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "49cc27aa-72e7-4e00-a992-9df2fcd9b2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1371980, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_id\n",
       "0  00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...\n",
       "1  0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...\n",
       "2  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...\n",
       "3  00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...\n",
       "4  00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f..."
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('sample_submission.csv').drop(\"prediction\", axis=1)\n",
    "print(test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ff90d5dc-ad6b-4ffb-bc34-ce9a9edf18ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>week</th>\n",
       "      <th>article_id</th>\n",
       "      <th>week_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[7154]</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[46435]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_id  week article_id  \\\n",
       "0  00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...    -1     [7154]   \n",
       "1  0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...    -1        NaN   \n",
       "2  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...    -1    [46435]   \n",
       "3  00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...    -1        NaN   \n",
       "4  00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f...    -1        NaN   \n",
       "\n",
       "  week_history  \n",
       "0          [2]  \n",
       "1          NaN  \n",
       "2          [1]  \n",
       "3          NaN  \n",
       "4          NaN  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_test_dataset(test_df):\n",
    "    week = -1\n",
    "    test_df[\"week\"] = week\n",
    "\n",
    "    hist_df = df[(df[\"week\"] > week) & (df[\"week\"] <= week + WEEK_HIST_MAX)]\n",
    "    hist_df = hist_df.groupby(\"customer_id\").agg({\"article_id\": list, \"week\": list}).reset_index()\n",
    "    hist_df.rename(columns={\"week\": 'week_history'}, inplace=True)\n",
    "\n",
    "\n",
    "    return test_df.merge(hist_df, on=\"customer_id\", how=\"left\")\n",
    "\n",
    "test_df = create_test_dataset(test_df)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fda775d6-c994-4ef4-a620-55df6ec15502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8008965145264508"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"article_id\"].isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bb0ec74f-7f96-4a96-9727-942dac7b6eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 42875/42875 [26:59<00:00, 26.48it/s]\n"
     ]
    }
   ],
   "source": [
    "test_ds = HMDataset(test_df, SEQ_LEN, is_test=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BS, shuffle=False, num_workers=NW,\n",
    "                          pin_memory=False, drop_last=False)\n",
    "\n",
    "\n",
    "def inference(model, loader, k=12):\n",
    "    model.eval()\n",
    "\n",
    "    tbar = tqdm(loader, file=sys.stdout)\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(tbar):\n",
    "            inputs, target = read_data(data)\n",
    "\n",
    "            logits = model(inputs)\n",
    "\n",
    "            _, indices = torch.topk(logits, k, dim=1)\n",
    "\n",
    "            indices = indices.detach().cpu().numpy()\n",
    "            target = target.detach().cpu().numpy()\n",
    "\n",
    "            for i in range(indices.shape[0]):\n",
    "                preds.append(\" \".join(list(le_article.inverse_transform(indices[i]))))\n",
    "\n",
    "\n",
    "    return preds\n",
    "\n",
    "# 테스트 데이터셋 예측 생성\n",
    "test_df[\"prediction\"] = inference(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0103f0f8-e404-4adb-94e4-71c1bf69d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 파일 생성 및 저장\n",
    "test_df.to_csv(\"submission.csv\", index=False, columns=[\"customer_id\", \"prediction\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
